Description

Predictive models are increasingly been deployed for the purpose of determining access to services such as credit, insurance, and employment. Despite societal gains in efficiency and productivity through deployment of these models, potential systemic flaws have not been fully addressed, particularly the potential for unintentional discrimination. This discrimination could be on the basis of race, gender, religion, sexual orientation, or other characteristics. This project addresses the question: how can an analyst determine the relative significance of the inputs to a black-box predictive model in order to assess the model’s fairness (or discriminatory extent)?

Data:
The data for this code is present in the following GitHub link:
/doc/example_notebooks/propublica_data_for_fairml.csv


The notebook works through the different stages of analysis starting from data exploration and modelling to analyzing final results in order to identify potential biases in the model based on race, age, sex, etc.




